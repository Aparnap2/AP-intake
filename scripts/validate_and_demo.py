#!/usr/bin/env python3
"""
Complete System Validation and Demo

Comprehensive validation of all test scenarios and execution of production demo.
This script validates the entire AP Intake & Validation system capabilities.
"""

import asyncio
import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.panel import Panel
from rich.align import Align

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.test_data_service import TestDataGenerator
from app.services.invoice_seeding_service import InvoiceSeedingService
from scripts.production_demo import ProductionDemo
from scripts.demo_report_generator import DemoReportGenerator


class SystemValidator:
    """Comprehensive system validator"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.console = Console()
        self.validation_results = {
            "validation_start": datetime.now().isoformat(),
            "test_data_generation": {},
            "database_seeding": {},
            "api_connectivity": {},
            "end_to_end_tests": {},
            "performance_tests": {},
            "demo_execution": {},
            "report_generation": {},
            "overall_status": "unknown",
            "success_rate": 0.0,
            "recommendations": []
        }

    async def run_complete_validation(self, run_demo: bool = True) -> Dict[str, Any]:
        """Run complete system validation"""
        self.console.print("\n")
        self.console.print(Align.center("[bold blue]ðŸ”¬ AP Intake & Validation - Complete System Validation[/bold blue]"))
        self.console.print(Rule(style="blue"))

        try:
            # Step 1: Test Data Generation Validation
            await self._validate_test_data_generation()

            # Step 2: Database Seeding Validation
            await self._validate_database_seeding()

            # Step 3: API Connectivity Validation
            await self._validate_api_connectivity()

            # Step 4: End-to-End Test Validation
            await self._validate_end_to_end_tests()

            # Step 5: Performance Test Validation
            await self._validate_performance_tests()

            # Step 6: Production Demo Execution (optional)
            if run_demo:
                await self._execute_production_demo()

            # Step 7: Report Generation Validation
            await self._validate_report_generation()

            # Step 8: Calculate Overall Results
            self._calculate_overall_results()

            # Step 9: Display Final Report
            self._display_final_report()

        except Exception as e:
            self.console.print(f"\nâŒ Validation failed: {str(e)}", style="red")
            self.validation_results["overall_status"] = "failed"
            self.validation_results["error"] = str(e)

        self.validation_results["validation_end"] = datetime.now().isoformat()
        return self.validation_results

    async def _validate_test_data_generation(self) -> None:
        """Validate test data generation capabilities"""
        self.console.print("\nðŸ”§ [bold]Step 1: Test Data Generation Validation[/bold]")

        with Progress(console=self.console) as progress:
            task = progress.add_task("Generating test data...", total=100)

            try:
                # Initialize test data generator
                generator = TestDataGenerator()
                progress.update(task, advance=20)

                # Generate comprehensive test data
                test_results = generator.generate_all_test_data()
                progress.update(task, advance=60)

                # Validate results
                total_scenarios = len(test_results.get("scenarios", {}))
                categories_present = len([k for k in test_results.keys() if k.endswith("_invoices") or k.endswith("_cases")])

                # Check required categories
                required_categories = ["standard_invoices", "duplicate_invoices", "exception_cases", "edge_cases", "performance_test"]
                categories_valid = all(cat in test_results for cat in required_categories)

                progress.update(task, advance=20)

                # Store results
                self.validation_results["test_data_generation"] = {
                    "status": "passed" if total_scenarios >= 50 and categories_valid else "failed",
                    "total_scenarios": total_scenarios,
                    "categories_present": categories_present,
                    "required_categories_present": categories_valid,
                    "categories": {cat: len(test_results.get(cat, [])) for cat in required_categories},
                    "details": test_results
                }

                if self.validation_results["test_data_generation"]["status"] == "passed":
                    self.console.print(f"âœ… Test data generation: [green]PASSED[/green] ({total_scenarios} scenarios)")
                else:
                    self.console.print(f"âŒ Test data generation: [red]FAILED[/red] (expected â‰¥50 scenarios, got {total_scenarios})")

            except Exception as e:
                self.console.print(f"âŒ Test data generation validation failed: {str(e)}", style="red")
                self.validation_results["test_data_generation"] = {
                    "status": "failed",
                    "error": str(e)
                }

    async def _validate_database_seeding(self) -> None:
        """Validate database seeding functionality"""
        self.console.print("\nðŸŒ± [bold]Step 2: Database Seeding Validation[/bold]")

        try:
            seeder = InvoiceSeedingService()

            # Clear existing test data
            self.console.print("ðŸ§¹ Clearing existing test data...")
            clear_result = await seeder.clear_test_data()

            # Seed with sample data
            self.console.print("ðŸ“Š Seeding database with test data...")
            seed_result = await seeder.seed_database(limit=10)

            if "error" not in seed_result:
                seeded_count = len(seed_result.get("seeded_scenarios", []))
                failed_count = len(seed_result.get("failed_scenarios", []))

                # Get statistics
                stats = await seeder.get_seeding_statistics()

                self.validation_results["database_seeding"] = {
                    "status": "passed" if seeded_count > 0 and failed_count == 0 else "partial",
                    "seeded_count": seeded_count,
                    "failed_count": failed_count,
                    "statistics": stats,
                    "success_rate": seeded_count / (seeded_count + failed_count) if (seeded_count + failed_count) > 0 else 0
                }

                if failed_count == 0:
                    self.console.print(f"âœ… Database seeding: [green]PASSED[/green] ({seeded_count} records)")
                else:
                    self.console.print(f"âš ï¸ Database seeding: [yellow]PARTIAL[/yellow] ({seeded_count} seeded, {failed_count} failed)")
            else:
                raise Exception(seed_result["error"])

        except Exception as e:
            self.console.print(f"âŒ Database seeding validation failed: {str(e)}", style="red")
            self.validation_results["database_seeding"] = {
                "status": "failed",
                "error": str(e)
            }

    async def _validate_api_connectivity(self) -> None:
        """Validate API connectivity and basic endpoints"""
        self.console.print("\nðŸŒ [bold]Step 3: API Connectivity Validation[/bold]")

        import httpx
        api_client = httpx.AsyncClient(base_url=self.base_url, timeout=10.0)

        try:
            endpoints_to_test = [
                ("/health", "Health Check"),
                ("/api/v1/info", "API Info"),
                ("/api/v1/invoices", "Invoice List"),
                ("/metrics", "Metrics"),
                ("/api/v1/validation/rules", "Validation Rules")
            ]

            results = {}
            passed_count = 0

            for endpoint, name in endpoints_to_test:
                try:
                    response = await api_client.get(endpoint)
                    status = "passed" if response.status_code in [200, 404] else "failed"  # 404 acceptable for some endpoints
                    results[name] = {
                        "endpoint": endpoint,
                        "status_code": response.status_code,
                        "status": status
                    }
                    if status == "passed":
                        passed_count += 1
                        self.console.print(f"  âœ… {name}: [green]{response.status_code}[/green]")
                    else:
                        self.console.print(f"  âŒ {name}: [red]{response.status_code}[/red]")

                except Exception as e:
                    results[name] = {
                        "endpoint": endpoint,
                        "status": "failed",
                        "error": str(e)
                    }
                    self.console.print(f"  âŒ {name}: [red]Connection failed[/red]")

            await api_client.aclose()

            self.validation_results["api_connectivity"] = {
                "status": "passed" if passed_count >= 3 else "failed",  # At least 3 endpoints must work
                "endpoints_tested": len(endpoints_to_test),
                "endpoints_passed": passed_count,
                "success_rate": passed_count / len(endpoints_to_test),
                "details": results
            }

            if passed_count >= 3:
                self.console.print(f"âœ… API connectivity: [green]PASSED[/green] ({passed_count}/{len(endpoints_to_test)} endpoints)")
            else:
                self.console.print(f"âŒ API connectivity: [red]FAILED[/red] ({passed_count}/{len(endpoints_to_test)} endpoints)")

        except Exception as e:
            await api_client.aclose()
            self.console.print(f"âŒ API connectivity validation failed: {str(e)}", style="red")
            self.validation_results["api_connectivity"] = {
                "status": "failed",
                "error": str(e)
            }

    async def _validate_end_to_end_tests(self) -> None:
        """Validate end-to-end test execution"""
        self.console.print("\nðŸ”„ [bold]Step 4: End-to-End Test Validation[/bold]")

        try:
            # Run pytest for integration tests
            import subprocess
            import tempfile

            with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as tmp_file:
                # Run pytest with JSON output
                result = subprocess.run([
                    sys.executable, "-m", "pytest",
                    "tests/integration/test_end_to_end.py::TestAPIEndpoints::test_health_check",
                    "tests/integration/test_end_to_end.py::TestAPIEndpoints::test_api_info",
                    "-v",
                    "--tb=short",
                    "--json-report",
                    f"--json-report-file={tmp_file.name}"
                ], capture_output=True, text=True, cwd=Path(__file__).parent.parent)

                # Parse results
                if result.returncode == 0:
                    test_passed = True
                    test_output = result.stdout
                else:
                    test_passed = False
                    test_output = result.stderr

                # Read JSON report if available
                try:
                    with open(tmp_file.name, 'r') as f:
                        json_report = json.load(f)
                        summary = json_report.get("summary", {})
                        total_tests = summary.get("total", 0)
                        passed_tests = summary.get("passed", 0)
                        failed_tests = summary.get("failed", 0)
                except:
                    # Fallback if JSON report not available
                    total_tests = 2  # We ran 2 tests
                    passed_tests = 2 if test_passed else 0
                    failed_tests = 0 if test_passed else 2

                self.validation_results["end_to_end_tests"] = {
                    "status": "passed" if test_passed else "failed",
                    "total_tests": total_tests,
                    "passed_tests": passed_tests,
                    "failed_tests": failed_tests,
                    "success_rate": passed_tests / total_tests if total_tests > 0 else 0,
                    "output": test_output[:500] + "..." if len(test_output) > 500 else test_output
                }

                if test_passed:
                    self.console.print(f"âœ… End-to-end tests: [green]PASSED[/green] ({passed_tests}/{total_tests} tests)")
                else:
                    self.console.print(f"âŒ End-to-end tests: [red]FAILED[/red] ({passed_tests}/{total_tests} tests)")

        except Exception as e:
            self.console.print(f"âŒ End-to-end test validation failed: {str(e)}", style="red")
            self.validation_results["end_to_end_tests"] = {
                "status": "failed",
                "error": str(e)
            }

    async def _validate_performance_tests(self) -> None:
        """Validate performance test execution"""
        self.console.print("\nâš¡ [bold]Step 5: Performance Test Validation[/bold]")

        try:
            # Simulate performance testing
            start_time = time.time()

            # Simulate processing multiple requests
            import httpx
            async with httpx.AsyncClient(base_url=self.base_url, timeout=5.0) as client:
                tasks = []
                for i in range(10):
                    tasks.append(client.get("/health"))
                    tasks.append(client.get("/api/v1/info"))

                results = await asyncio.gather(*tasks, return_exceptions=True)

            end_time = time.time()
            total_time = end_time - start_time

            # Analyze results
            successful_requests = sum(1 for r in results if not isinstance(r, Exception) and r.status_code == 200)
            total_requests = len(results)
            throughput = total_requests / total_time

            self.validation_results["performance_tests"] = {
                "status": "passed" if throughput > 2.0 else "failed",
                "total_requests": total_requests,
                "successful_requests": successful_requests,
                "total_time": total_time,
                "throughput": throughput,
                "success_rate": successful_requests / total_requests
            }

            if throughput > 2.0:
                self.console.print(f"âœ… Performance tests: [green]PASSED[/green] ({throughput:.1f} req/s)")
            else:
                self.console.print(f"âŒ Performance tests: [red]FAILED[/red] ({throughput:.1f} req/s, expected >2.0)")

        except Exception as e:
            self.console.print(f"âŒ Performance test validation failed: {str(e)}", style="red")
            self.validation_results["performance_tests"] = {
                "status": "failed",
                "error": str(e)
            }

    async def _execute_production_demo(self) -> None:
        """Execute production demo"""
        self.console.print("\nðŸŽ¬ [bold]Step 6: Production Demo Execution[/bold]")

        try:
            demo = ProductionDemo(base_url=self.base_url, demo_mode="quick")
            demo_results = await demo.run_demo()

            success_rate = demo_results.get("success_rate", 0)

            self.validation_results["demo_execution"] = {
                "status": "passed" if success_rate >= 75 else "failed",
                "success_rate": success_rate,
                "demo_results": demo_results
            }

            if success_rate >= 75:
                self.console.print(f"âœ… Production demo: [green]PASSED[/green] ({success_rate:.1f}% success rate)")
            else:
                self.console.print(f"âŒ Production demo: [red]FAILED[/red] ({success_rate:.1f}% success rate)")

        except Exception as e:
            self.console.print(f"âŒ Production demo execution failed: {str(e)}", style="red")
            self.validation_results["demo_execution"] = {
                "status": "failed",
                "error": str(e)
            }

    async def _validate_report_generation(self) -> None:
        """Validate report generation capabilities"""
        self.console.print("\nðŸ“Š [bold]Step 7: Report Generation Validation[/bold]")

        try:
            generator = DemoReportGenerator(base_url=self.base_url)
            report_results = await generator.generate_comprehensive_report(output_dir="validation_reports")

            self.validation_results["report_generation"] = {
                "status": "passed",
                "report_files": report_results,
                "html_report_generated": Path(report_results["html_report"]).exists(),
                "json_report_generated": Path(report_results["json_report"]).exists(),
                "summary_report_generated": Path(report_results["summary_report"]).exists()
            }

            self.console.print(f"âœ… Report generation: [green]PASSED[/green]")
            self.console.print(f"   ðŸ“„ HTML Report: {report_results['html_report']}")
            self.console.print(f"   ðŸ“„ JSON Data: {report_results['json_report']}")
            self.console.print(f"   ðŸ“„ Summary: {report_results['summary_report']}")

        except Exception as e:
            self.console.print(f"âŒ Report generation validation failed: {str(e)}", style="red")
            self.validation_results["report_generation"] = {
                "status": "failed",
                "error": str(e)
            }

    def _calculate_overall_results(self) -> None:
        """Calculate overall validation results"""
        validation_areas = [
            "test_data_generation",
            "database_seeding",
            "api_connectivity",
            "end_to_end_tests",
            "performance_tests",
            "demo_execution",
            "report_generation"
        ]

        passed_areas = 0
        total_areas = 0
        weighted_score = 0

        weights = {
            "test_data_generation": 0.15,
            "database_seeding": 0.15,
            "api_connectivity": 0.20,
            "end_to_end_tests": 0.20,
            "performance_tests": 0.10,
            "demo_execution": 0.15,
            "report_generation": 0.05
        }

        for area in validation_areas:
            if area in self.validation_results:
                total_areas += 1
                area_result = self.validation_results[area]
                status = area_result.get("status", "failed")

                if status == "passed":
                    passed_areas += 1
                    weighted_score += weights.get(area, 0.1)
                elif status == "partial":
                    weighted_score += weights.get(area, 0.1) * 0.5

        # Calculate overall success rate
        overall_success_rate = (passed_areas / total_areas * 100) if total_areas > 0 else 0

        # Determine overall status
        if overall_success_rate >= 90 and weighted_score >= 0.8:
            overall_status = "excellent"
        elif overall_success_rate >= 75 and weighted_score >= 0.6:
            overall_status = "good"
        elif overall_success_rate >= 50:
            overall_status = "needs_improvement"
        else:
            overall_status = "failed"

        self.validation_results.update({
            "overall_status": overall_status,
            "success_rate": overall_success_rate,
            "weighted_score": weighted_score,
            "areas_passed": passed_areas,
            "total_areas": total_areas
        })

    def _display_final_report(self) -> None:
        """Display final validation report"""
        self.console.print("\n")
        self.console.print(Align.center("[bold blue]ðŸŽ¯ VALIDATION FINAL REPORT[/bold blue]"))
        self.console.print(Rule(style="blue"))

        # Overall status
        overall_status = self.validation_results["overall_status"]
        success_rate = self.validation_results["success_rate"]
        weighted_score = self.validation_results["weighted_score"]

        status_colors = {
            "excellent": "green",
            "good": "blue",
            "needs_improvement": "yellow",
            "failed": "red"
        }
        status_color = status_colors.get(overall_status, "white")

        self.console.print(f"\nðŸ† Overall Status: [{status_color}]{overall_status.upper()}[/{status_color}]")
        self.console.print(f"ðŸ“Š Success Rate: {success_rate:.1f}%")
        self.console.print(f"âš–ï¸ Weighted Score: {weighted_score:.2f}")

        # Detailed results table
        results_table = Table(title="Validation Results by Area", show_header=True, header_style="bold blue")
        results_table.add_column("Area", style="cyan")
        results_table.add_column("Status", justify="center")
        results_table.add_column("Score", justify="right", style="green")
        results_table.add_column("Details", style="white")

        areas_info = {
            "test_data_generation": ("Test Data Generation", "scenarios"),
            "database_seeding": ("Database Seeding", "records"),
            "api_connectivity": ("API Connectivity", "endpoints"),
            "end_to_end_tests": ("End-to-End Tests", "tests"),
            "performance_tests": ("Performance Tests", "req/s"),
            "demo_execution": ("Production Demo", "% success"),
            "report_generation": ("Report Generation", "files")
        }

        for area, (name, unit) in areas_info.items():
            if area in self.validation_results:
                result = self.validation_results[area]
                status = result.get("status", "failed")
                status_symbol = "âœ…" if status == "passed" else "âš ï¸" if status == "partial" else "âŒ"

                # Get score details
                if area == "test_data_generation":
                    score = result.get("total_scenarios", 0)
                elif area == "database_seeding":
                    score = result.get("seeded_count", 0)
                elif area == "api_connectivity":
                    score = f"{result.get('endpoints_passed', 0)}/{result.get('endpoints_tested', 0)}"
                elif area == "end_to_end_tests":
                    score = f"{result.get('passed_tests', 0)}/{result.get('total_tests', 0)}"
                elif area == "performance_tests":
                    score = f"{result.get('throughput', 0):.1f}"
                elif area == "demo_execution":
                    score = f"{result.get('success_rate', 0):.1f}"
                elif area == "report_generation":
                    score = "3/3"  # 3 report files
                else:
                    score = "N/A"

                results_table.add_row(
                    name,
                    f"{status_symbol} {status.title()}",
                    str(score),
                    f"Unit: {unit}"
                )

        self.console.print(results_table)

        # Recommendations
        self._generate_recommendations()
        if self.validation_results["recommendations"]:
            self.console.print("\nðŸ’¡ Recommendations:")
            for rec in self.validation_results["recommendations"]:
                priority_color = {"high": "red", "medium": "yellow", "low": "green"}.get(rec["priority"], "white")
                self.console.print(f"  â€¢ [{priority_color}][{rec['priority'].upper()}][/{priority_color}] {rec['title']}")
                self.console.print(f"    {rec['description']}")

        # Final assessment
        self.console.print(f"\n{Rule(style='blue')}")
        if overall_status == "excellent":
            self.console.print(Align.center("[bold green]ðŸš€ SYSTEM IS PRODUCTION READY! ðŸš€[/bold green]"))
        elif overall_status == "good":
            self.console.print(Align.center("[bold blue]âœ… SYSTEM IS READY WITH MINOR IMPROVEMENTS[/bold blue]"))
        elif overall_status == "needs_improvement":
            self.console.print(Align.center("[bold yellow]âš ï¸ SYSTEM NEEDS IMPROVEMENTS BEFORE PRODUCTION[/bold yellow]"))
        else:
            self.console.print(Align.center("[bold red]âŒ SYSTEM IS NOT READY FOR PRODUCTION[/bold red]"))

        # Save validation report
        report_file = Path("validation_report.json")
        with open(report_file, 'w') as f:
            json.dump(self.validation_results, f, indent=2, default=str)
        self.console.print(f"\nðŸ“„ Detailed validation report saved to: {report_file}")

    def _generate_recommendations(self) -> None:
        """Generate recommendations based on validation results"""
        recommendations = []

        # Check each area for recommendations
        if self.validation_results.get("test_data_generation", {}).get("status") != "passed":
            recommendations.append({
                "priority": "high",
                "title": "Fix Test Data Generation",
                "description": "Test data generation is not working properly. Ensure at least 50 test scenarios are generated across all required categories."
            })

        if self.validation_results.get("api_connectivity", {}).get("status") != "passed":
            recommendations.append({
                "priority": "high",
                "title": "Fix API Connectivity",
                "description": "API endpoints are not responding correctly. Ensure the API server is running and all critical endpoints are accessible."
            })

        if self.validation_results.get("performance_tests", {}).get("status") != "passed":
            recommendations.append({
                "priority": "medium",
                "title": "Improve Performance",
                "description": "System performance is below expectations. Consider optimizing database queries, adding caching, or increasing resources."
            })

        if self.validation_results.get("demo_execution", {}).get("status") != "passed":
            recommendations.append({
                "priority": "medium",
                "title": "Fix Production Demo",
                "description": "Production demo execution failed. Review demo configuration and ensure all system components are working properly."
            })

        success_rate = self.validation_results.get("success_rate", 0)
        if success_rate < 100:
            recommendations.append({
                "priority": "low",
                "title": "Address Minor Issues",
                "description": f"System validation achieved {success_rate:.1f}% success rate. Address remaining issues to achieve 100% validation."
            })

        if not recommendations:
            recommendations.append({
                "priority": "low",
                "title": "System Validation Complete",
                "description": "All validation checks passed. System is ready for production deployment."
            })

        self.validation_results["recommendations"] = recommendations


async def main():
    """Main validation execution"""
    parser = argparse.ArgumentParser(description="Complete system validation and demo")
    parser.add_argument("--base-url", default="http://localhost:8000", help="Base URL for API")
    parser.add_argument("--no-demo", action="store_true", help="Skip production demo execution")
    parser.add_argument("--output", help="Output directory for reports")

    args = parser.parse_args()

    console = Console()
    console.print("ðŸ”¬ Starting complete system validation...")

    validator = SystemValidator(base_url=args.base_url)
    results = await validator.run_complete_validation(run_demo=not args.no_demo)

    # Exit with appropriate code based on results
    overall_status = results.get("overall_status", "failed")
    if overall_status == "excellent":
        console.print("âœ… System validation completed successfully!")
        sys.exit(0)
    elif overall_status == "good":
        console.print("âœ… System validation completed with minor issues")
        sys.exit(0)
    elif overall_status == "needs_improvement":
        console.print("âš ï¸ System validation completed with issues that need attention")
        sys.exit(1)
    else:
        console.print("âŒ System validation failed")
        sys.exit(2)


if __name__ == "__main__":
    asyncio.run(main())