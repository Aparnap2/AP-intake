#!/usr/bin/env python3
"""
Advanced Database Testing Suite for AP Intake & Validation System

This suite performs advanced testing including concurrent access,
index performance, backup testing, and scalability analysis.
"""

import asyncio
import json
import time
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import statistics
import concurrent.futures
import threading
from contextlib import contextmanager

import asyncpg
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# Get project root directory
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Database configuration
ASYNC_DB_URL = "postgresql://postgres:postgres@localhost:5432/ap_intake"
SYNC_DB_URL = "postgresql://postgres:postgres@localhost:5432/ap_intake"

class AdvancedDatabaseTestSuite:
    """Advanced database testing suite for production readiness."""

    def __init__(self):
        self.results = {
            'concurrent_load': {},
            'index_analysis': {},
            'scalability': {},
            'backup_recovery': {},
            'connection_stress': {},
            'deadlock_detection': {},
            'transaction_isolation': {}
        }
        self.test_start_time = time.time()

    async def run_all_tests(self):
        """Run all advanced database tests."""
        print("üöÄ Starting Advanced Database Testing Suite")
        print("=" * 60)

        try:
            # Run advanced test categories
            await self.test_concurrent_load()
            await self.test_index_analysis()
            await self.test_scalability()
            await self.test_connection_stress()
            await self.test_deadlock_detection()
            await self.test_transaction_isolation()
            await self.test_backup_recovery_simulation()

            # Generate comprehensive report
            self._generate_final_report()

        except Exception as e:
            print(f"‚ùå Advanced test suite execution failed: {e}")
            self._generate_final_report()

    async def test_concurrent_load(self):
        """Test database under high concurrent load."""
        print("\n‚ö° Testing Concurrent Load...")

        start_time = time.time()

        try:
            # Create connection pool
            pool = await asyncpg.create_pool(
                ASYNC_DB_URL,
                min_size=5,
                max_size=50,
                command_timeout=10.0
            )

            try:
                load_results = {}

                # Test 1: Concurrent Reads
                print("  ‚Ä¢ Testing concurrent reads...")
                read_results = await self._test_concurrent_reads(pool)
                load_results['concurrent_reads'] = read_results

                # Test 2: Concurrent Writes
                print("  ‚Ä¢ Testing concurrent writes...")
                write_results = await self._test_concurrent_writes(pool)
                load_results['concurrent_writes'] = write_results

                # Test 3: Mixed Operations
                print("  ‚Ä¢ Testing mixed concurrent operations...")
                mixed_results = await self._test_mixed_operations(pool)
                load_results['mixed_operations'] = mixed_results

                # Calculate overall load score
                avg_throughput = statistics.mean([
                    read_results.get('throughput', 0),
                    write_results.get('throughput', 0),
                    mixed_results.get('throughput', 0)
                ])

                success_rate = (
                    read_results.get('success_rate', 0) +
                    write_results.get('success_rate', 0) +
                    mixed_results.get('success_rate', 0)
                ) / 3

                self.results['concurrent_load'] = {
                    'status': 'PASSED' if success_rate > 95 else 'FAILED',
                    'duration': time.time() - start_time,
                    'load_results': load_results,
                    'average_throughput': avg_throughput,
                    'overall_success_rate': success_rate
                }

                print(f"‚úÖ Concurrent load test: {success_rate:.1f}% success rate")
                print(f"   ‚Ä¢ Average throughput: {avg_throughput:.1f} ops/sec")

            finally:
                await pool.close()

        except Exception as e:
            self.results['concurrent_load'] = {
                'status': 'FAILED',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            print(f"‚ùå Concurrent load test failed: {e}")

    async def _test_concurrent_reads(self, pool):
        """Test concurrent read operations."""
        async def read_worker(worker_id):
            operations = 0
            start_time = time.time()

            async with pool.acquire() as conn:
                for i in range(50):
                    try:
                        # Various read operations
                        await conn.fetchval("SELECT COUNT(*) FROM vendors")
                        await conn.fetch("SELECT * FROM vendors WHERE active = TRUE LIMIT 10")
                        await conn.fetch("SELECT * FROM invoices LIMIT 5")
                        operations += 3
                    except Exception:
                        pass

            return {
                'worker_id': worker_id,
                'operations': operations,
                'duration': time.time() - start_time
            }

        # Run 20 concurrent read workers
        start_time = time.time()
        tasks = [read_worker(i) for i in range(20)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_duration = time.time() - start_time

        # Calculate metrics
        successful_workers = [r for r in results if not isinstance(r, Exception)]
        total_operations = sum(r['operations'] for r in successful_workers)
        success_rate = len(successful_workers) / 20 * 100

        return {
            'workers': 20,
            'successful_workers': len(successful_workers),
            'total_operations': total_operations,
            'duration': total_duration,
            'throughput': total_operations / total_duration,
            'success_rate': success_rate
        }

    async def _test_concurrent_writes(self, pool):
        """Test concurrent write operations."""
        async def write_worker(worker_id):
            operations = 0
            created_ids = []
            start_time = time.time()

            async with pool.acquire() as conn:
                async with conn.transaction():
                    try:
                        for i in range(10):
                            # Create vendor
                            vendor_id = uuid.uuid4()
                            await conn.execute("""
                                INSERT INTO vendors (id, name, currency, active, status)
                                VALUES ($1, $2, $3, $4, $5)
                            """, vendor_id, f"Concurrent Vendor {worker_id}-{i}", "USD", True, "active")
                            created_ids.append(vendor_id)

                            # Create invoice
                            invoice_id = uuid.uuid4()
                            await conn.execute("""
                                INSERT INTO invoices (id, vendor_id, file_url, file_hash, file_name, file_size, status)
                                VALUES ($1, $2, $3, $4, $5, $6, $7)
                            """, invoice_id, vendor_id, f"file_{worker_id}_{i}.pdf", f"hash_{worker_id}_{i}",
                                f"file_{worker_id}_{i}.pdf", "1MB", "received")
                            operations += 2

                        # Clean up created data
                        await conn.execute("DELETE FROM invoices WHERE vendor_id = ANY($1)", created_ids)
                        await conn.execute("DELETE FROM vendors WHERE id = ANY($1)", created_ids)

                    except Exception as e:
                        # Clean up on error
                        if created_ids:
                            await conn.execute("DELETE FROM invoices WHERE vendor_id = ANY($1)", created_ids)
                            await conn.execute("DELETE FROM vendors WHERE id = ANY($1)", created_ids)
                        raise e

            return {
                'worker_id': worker_id,
                'operations': operations,
                'duration': time.time() - start_time
            }

        # Run 10 concurrent write workers
        start_time = time.time()
        tasks = [write_worker(i) for i in range(10)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_duration = time.time() - start_time

        # Calculate metrics
        successful_workers = [r for r in results if not isinstance(r, Exception)]
        total_operations = sum(r['operations'] for r in successful_workers)
        success_rate = len(successful_workers) / 10 * 100

        return {
            'workers': 10,
            'successful_workers': len(successful_workers),
            'total_operations': total_operations,
            'duration': total_duration,
            'throughput': total_operations / total_duration,
            'success_rate': success_rate
        }

    async def _test_mixed_operations(self, pool):
        """Test mixed concurrent operations."""
        async def mixed_worker(worker_id):
            read_ops = 0
            write_ops = 0
            start_time = time.time()
            created_ids = []

            async with pool.acquire() as conn:
                try:
                    for i in range(20):
                        # Mix of read and write operations
                        if i % 3 == 0:
                            # Write operation
                            vendor_id = uuid.uuid4()
                            await conn.execute("""
                                INSERT INTO vendors (id, name, currency, active, status)
                                VALUES ($1, $2, $3, $4, $5)
                            """, vendor_id, f"Mixed Vendor {worker_id}-{i}", "USD", True, "active")
                            created_ids.append(vendor_id)
                            write_ops += 1
                        else:
                            # Read operation
                            await conn.fetchval("SELECT COUNT(*) FROM vendors")
                            await conn.fetch("SELECT * FROM vendors LIMIT 5")
                            read_ops += 2

                    # Clean up
                    if created_ids:
                        await conn.execute("DELETE FROM vendors WHERE id = ANY($1)", created_ids)

                except Exception:
                    # Clean up on error
                    if created_ids:
                        await conn.execute("DELETE FROM vendors WHERE id = ANY($1)", created_ids)
                    raise

            return {
                'worker_id': worker_id,
                'read_operations': read_ops,
                'write_operations': write_ops,
                'total_operations': read_ops + write_ops,
                'duration': time.time() - start_time
            }

        # Run 15 mixed workers
        start_time = time.time()
        tasks = [mixed_worker(i) for i in range(15)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_duration = time.time() - start_time

        # Calculate metrics
        successful_workers = [r for r in results if not isinstance(r, Exception)]
        total_operations = sum(r['total_operations'] for r in successful_workers)
        success_rate = len(successful_workers) / 15 * 100

        return {
            'workers': 15,
            'successful_workers': len(successful_workers),
            'total_operations': total_operations,
            'duration': total_duration,
            'throughput': total_operations / total_duration,
            'success_rate': success_rate
        }

    async def test_index_analysis(self):
        """Test index usage and performance."""
        print("\nüìä Testing Index Analysis...")

        start_time = time.time()

        try:
            conn = await asyncpg.connect(ASYNC_DB_URL)

            try:
                index_results = {}

                # Test 1: Index Usage Analysis
                print("  ‚Ä¢ Analyzing index usage...")
                usage_analysis = await self._analyze_index_usage(conn)
                index_results['usage_analysis'] = usage_analysis

                # Test 2: Index Performance Impact
                print("  ‚Ä¢ Testing index performance impact...")
                performance_impact = await self._test_index_performance_impact(conn)
                index_results['performance_impact'] = performance_impact

                # Test 3: Index Health Check
                print("  ‚Ä¢ Checking index health...")
                health_check = await self._check_index_health(conn)
                index_results['health_check'] = health_check

                self.results['index_analysis'] = {
                    'status': 'PASSED',
                    'duration': time.time() - start_time,
                    'index_results': index_results
                }

                print(f"‚úÖ Index analysis completed")
                print(f"   ‚Ä¢ Indexes analyzed: {usage_analysis.get('total_indexes', 0)}")
                print(f"   ‚Ä¢ Performance impact: {performance_impact.get('overall_improvement', 0):.1f}%")

            finally:
                await conn.close()

        except Exception as e:
            self.results['index_analysis'] = {
                'status': 'FAILED',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            print(f"‚ùå Index analysis test failed: {e}")

    async def _analyze_index_usage(self, conn):
        """Analyze current index usage."""
        # Get index information
        indexes = await conn.fetch("""
            SELECT
                schemaname,
                tablename,
                indexname,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch,
                pg_size_pretty(pg_relation_size(indexrelid)) as index_size
            FROM pg_stat_user_indexes
            WHERE schemaname = 'public'
            ORDER BY idx_scan DESC
        """)

        # Analyze unused indexes
        unused_indexes = [idx for idx in indexes if idx['idx_scan'] == 0]

        # Analyze heavily used indexes
        heavily_used = [idx for idx in indexes if idx['idx_scan'] > 100]

        return {
            'total_indexes': len(indexes),
            'unused_indexes': len(unused_indexes),
            'heavily_used_indexes': len(heavily_used),
            'unused_index_details': [dict(idx) for idx in unused_indexes],
            'heavily_used_details': [dict(idx) for idx in heavily_used],
            'all_indexes': [dict(idx) for idx in indexes]
        }

    async def _test_index_performance_impact(self, conn):
        """Test performance impact of indexes."""
        # Create temporary test table
        await conn.execute("""
            CREATE TEMPORARY TABLE index_test_table (
                id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                name VARCHAR(100),
                category VARCHAR(50),
                active BOOLEAN,
                value NUMERIC,
                created_at TIMESTAMP DEFAULT NOW()
            )
        """)

        try:
            # Insert test data
            test_data = []
            for i in range(1000):
                await conn.execute("""
                    INSERT INTO index_test_table (name, category, active, value)
                    VALUES ($1, $2, $3, $4)
                """, f"Test Item {i}", f"Category {i % 10}", i % 2 == 0, i * 1.5)

            # Test queries without indexes
            start_time = time.time()
            for _ in range(100):
                await conn.fetch("""
                    SELECT * FROM index_test_table WHERE active = TRUE LIMIT 10
                """)
            no_index_time = time.time() - start_time

            # Create indexes
            await conn.execute("CREATE INDEX temp_idx_active ON index_test_table(active)")
            await conn.execute("CREATE INDEX temp_idx_category ON index_test_table(category)")

            # Test queries with indexes
            start_time = time.time()
            for _ in range(100):
                await conn.fetch("""
                    SELECT * FROM index_test_table WHERE active = TRUE LIMIT 10
                """)
            with_index_time = time.time() - start_time

            # Calculate improvement
            improvement = ((no_index_time - with_index_time) / no_index_time) * 100

            return {
                'query_without_index_ms': (no_index_time / 100) * 1000,
                'query_with_index_ms': (with_index_time / 100) * 1000,
                'performance_improvement_percent': improvement,
                'overall_improvement': improvement
            }

        finally:
            await conn.execute("DROP TABLE index_test_table")

    async def _check_index_health(self, conn):
        """Check index health and statistics."""
        # Get index statistics
        index_stats = await conn.fetch("""
            SELECT
                n.nspname as schema_name,
                c.relname as table_name,
                i.relname as index_name,
                pg_size_pretty(pg_relation_size(i.oid)) as index_size,
                pg_stat_get_numscans(i.oid) as scans,
                pg_stat_get_tuples_returned(i.oid) as tuples_returned,
                pg_stat_get_tuples_fetched(i.oid) as tuples_fetched
            FROM pg_class i
            JOIN pg_index ix ON i.oid = ix.indexrelid
            JOIN pg_class c ON ix.indrelid = c.oid
            JOIN pg_namespace n ON n.oid = c.relnamespace
            WHERE n.nspname = 'public'
            ORDER BY pg_relation_size(i.oid) DESC
        """)

        # Check for bloated indexes (simplified check)
        bloated_indexes = []
        for stat in index_stats:
            if stat['scans'] == 0 and stat['index_size'] != '8192 bytes':
                bloated_indexes.append(dict(stat))

        return {
            'total_indexes': len(index_stats),
            'bloated_indexes': len(bloated_indexes),
            'total_index_size': sum(
                int(stat['index_size'].split()[0]) for stat in index_stats
                if 'MB' in stat['index_size']
            ),
            'index_details': [dict(stat) for stat in index_stats],
            'bloated_details': bloated_indexes
        }

    async def test_scalability(self):
        """Test database scalability under increasing load."""
        print("\nüìà Testing Database Scalability...")

        start_time = time.time()

        try:
            scalability_results = {}

            # Test different load levels
            load_levels = [10, 50, 100, 200]  # Number of concurrent operations

            for load_level in load_levels:
                print(f"  ‚Ä¢ Testing with {load_level} concurrent operations...")
                result = await self._test_load_level(load_level)
                scalability_results[f'load_{load_level}'] = result

            # Analyze scalability trends
            scalability_analysis = self._analyze_scalability_trends(scalability_results)

            self.results['scalability'] = {
                'status': 'PASSED',
                'duration': time.time() - start_time,
                'scalability_results': scalability_results,
                'analysis': scalability_analysis
            }

            print(f"‚úÖ Scalability test completed")
            print(f"   ‚Ä¢ Max tested load: {max(load_levels)} concurrent operations")
            print(f"   ‚Ä¢ Performance degradation: {scalability_analysis.get('performance_degradation', 0):.1f}%")

        except Exception as e:
            self.results['scalability'] = {
                'status': 'FAILED',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            print(f"‚ùå Scalability test failed: {e}")

    async def _test_load_level(self, concurrency):
        """Test specific load level."""
        pool = await asyncpg.create_pool(
            ASYNC_DB_URL,
            min_size=5,
            max_size=concurrency + 10,
            command_timeout=15.0
        )

        try:
            async def load_worker(worker_id):
                operations = 0
                start_time = time.time()

                async with pool.acquire() as conn:
                    for i in range(20):
                        try:
                            # Simple query
                            await conn.fetchval("SELECT COUNT(*) FROM vendors")
                            operations += 1
                        except Exception:
                            pass

                return {
                    'worker_id': worker_id,
                    'operations': operations,
                    'duration': time.time() - start_time
                }

            # Run workers
            start_time = time.time()
            tasks = [load_worker(i) for i in range(concurrency)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            total_duration = time.time() - start_time

            # Calculate metrics
            successful_workers = [r for r in results if not isinstance(r, Exception)]
            total_operations = sum(r['operations'] for r in successful_workers)

            return {
                'concurrency': concurrency,
                'successful_workers': len(successful_workers),
                'total_operations': total_operations,
                'duration': total_duration,
                'throughput': total_operations / total_duration,
                'success_rate': len(successful_workers) / concurrency * 100,
                'avg_response_time': total_duration / total_operations if total_operations > 0 else 0
            }

        finally:
            await pool.close()

    def _analyze_scalability_trends(self, results):
        """Analyze scalability trends across load levels."""
        loads = []
        throughputs = []

        for key, result in results.items():
            if isinstance(result, dict):
                loads.append(result['concurrency'])
                throughputs.append(result['throughput'])

        if len(loads) < 2:
            return {'performance_degradation': 0}

        # Calculate performance degradation
        base_throughput = throughputs[0]
        max_throughput = max(throughputs)
        min_throughput = min(throughputs[-2:])  # Last two measurements

        degradation = ((base_throughput - min_throughput) / base_throughput) * 100

        return {
            'performance_degradation': degradation,
            'peak_throughput': max_throughput,
            'base_throughput': base_throughput,
            'scalability_factor': max_throughput / base_throughput if base_throughput > 0 else 0
        }

    async def test_connection_stress(self):
        """Test connection pool under stress."""
        print("\nüîó Testing Connection Stress...")

        start_time = time.time()

        try:
            # Test 1: Rapid connection acquisition/release
            print("  ‚Ä¢ Testing rapid connection cycling...")
            rapid_cycling = await self._test_rapid_connection_cycling()

            # Test 2: Connection pool exhaustion
            print("  ‚Ä¢ Testing connection pool exhaustion...")
            pool_exhaustion = await self._test_pool_exhaustion()

            # Test 3: Connection timeout handling
            print("  ‚Ä¢ Testing connection timeout handling...")
            timeout_handling = await self._test_connection_timeout()

            self.results['connection_stress'] = {
                'status': 'PASSED',
                'duration': time.time() - start_time,
                'rapid_cycling': rapid_cycling,
                'pool_exhaustion': pool_exhaustion,
                'timeout_handling': timeout_handling
            }

            print(f"‚úÖ Connection stress test completed")
            print(f"   ‚Ä¢ Rapid cycling: {rapid_cycling.get('cycles', 0)} cycles")
            print(f"   ‚Ä¢ Pool exhaustion handled: {pool_exhaustion.get('handled', False)}")

        except Exception as e:
            self.results['connection_stress'] = {
                'status': 'FAILED',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            print(f"‚ùå Connection stress test failed: {e}")

    async def _test_rapid_connection_cycling(self):
        """Test rapid connection acquisition and release."""
        pool = await asyncpg.create_pool(
            ASYNC_DB_URL,
            min_size=2,
            max_size=20
        )

        try:
            cycles = 0
            start_time = time.time()

            # Run for 5 seconds
            while time.time() - start_time < 5:
                async with pool.acquire() as conn:
                    await conn.fetchval("SELECT 1")
                cycles += 1

            return {
                'cycles': cycles,
                'duration': time.time() - start_time,
                'cycles_per_second': cycles / (time.time() - start_time)
            }

        finally:
            await pool.close()

    async def _test_pool_exhaustion(self):
        """Test behavior when connection pool is exhausted."""
        pool = await asyncpg.create_pool(
            ASYNC_DB_URL,
            min_size=1,
            max_size=3
        )

        try:
            # Acquire all connections
            connections = []
            for i in range(3):
                conn = await pool.acquire()
                connections.append(conn)

            # Try to acquire one more (should timeout)
            try:
                async with asyncio.timeout(2.0):
                    conn = await pool.acquire()
                    await pool.release(conn)
                    return {'handled': False, 'reason': 'Should have timed out'}
            except asyncio.TimeoutError:
                return {'handled': True, 'timeout_occurred': True}
            finally:
                # Release held connections
                for conn in connections:
                    await pool.release(conn)

        finally:
            await pool.close()

    async def _test_connection_timeout(self):
        """Test connection timeout handling."""
        try:
            # Create pool with short timeout
            pool = await asyncpg.create_pool(
                ASYNC_DB_URL,
                min_size=1,
                max_size=2,
                command_timeout=1.0
            )

            try:
                # Test normal query
                async with pool.acquire() as conn:
                    await conn.fetchval("SELECT 1")

                # Test long query (should timeout)
                try:
                    async with pool.acquire() as conn:
                        await conn.fetchval("SELECT pg_sleep(2)")
                    return {'handled': False, 'reason': 'Long query should have timed out'}
                except Exception:
                    return {'handled': True, 'timeout_working': True}

            finally:
                await pool.close()

        except Exception as e:
            return {'handled': False, 'error': str(e)}

    async def test_deadlock_detection(self):
        """Test deadlock detection and handling."""
        print("\nüîÄ Testing Deadlock Detection...")

        start_time = time.time()

        try:
            deadlock_results = await self._simulate_deadlock_scenario()

            self.results['deadlock_detection'] = {
                'status': 'PASSED',
                'duration': time.time() - start_time,
                'deadlock_results': deadlock_results
            }

            print(f"‚úÖ Deadlock detection test completed")
            print(f"   ‚Ä¢ Deadlocks detected: {deadlock_results.get('deadlocks_detected', 0)}")

        except Exception as e:
            self.results['deadlock_detection'] = {
                'status': 'FAILED',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            print(f"‚ùå Deadlock detection test failed: {e}")

    async def _simulate_deadlock_scenario(self):
        """Simulate a potential deadlock scenario."""
        async def transaction1():
            conn = await asyncpg.connect(ASYNC_DB_URL)
            try:
                async with conn.transaction():
                    # Lock vendors table
                    await conn.execute("LOCK TABLE vendors IN SHARE ROW EXCLUSIVE MODE")
                    await asyncio.sleep(0.1)

                    # Try to lock invoices table
                    await conn.execute("LOCK TABLE invoices IN SHARE ROW EXCLUSIVE MODE")
                return "success"
            except Exception as e:
                return f"deadlock: {str(e)}"
            finally:
                await conn.close()

        async def transaction2():
            conn = await asyncpg.connect(ASYNC_DB_URL)
            try:
                async with conn.transaction():
                    # Lock invoices table first
                    await conn.execute("LOCK TABLE invoices IN SHARE ROW EXCLUSIVE MODE")
                    await asyncio.sleep(0.05)

                    # Try to lock vendors table
                    await conn.execute("LOCK TABLE vendors IN SHARE ROW EXCLUSIVE MODE")
                return "success"
            except Exception as e:
                return f"deadlock: {str(e)}"
            finally:
                await conn.close()

        # Run transactions concurrently
        start_time = time.time()
        results = await asyncio.gather(
            transaction1(),
            transaction2(),
            return_exceptions=True
        )
        duration = time.time() - start_time

        deadlocks_detected = sum(1 for r in results if isinstance(r, str) and "deadlock" in r.lower())

        return {
            'transactions_run': 2,
            'deadlocks_detected': deadlocks_detected,
            'duration': duration,
            'results': results
        }

    async def test_transaction_isolation(self):
        """Test transaction isolation levels."""
        print("\nüîí Testing Transaction Isolation...")

        start_time = time.time()

        try:
            isolation_results = {}

            # Test 1: Read Committed Isolation
            print("  ‚Ä¢ Testing READ COMMITTED isolation...")
            read_committed = await self._test_read_committed_isolation()

            # Test 2: Repeatable Read (if supported)
            print("  ‚Ä¢ Testing REPEATABLE READ isolation...")
            repeatable_read = await self._test_repeatable_read_isolation()

            isolation_results['read_committed'] = read_committed
            isolation_results['repeatable_read'] = repeatable_read

            self.results['transaction_isolation'] = {
                'status': 'PASSED',
                'duration': time.time() - start_time,
                'isolation_results': isolation_results
            }

            print(f"‚úÖ Transaction isolation test completed")

        except Exception as e:
            self.results['transaction_isolation'] = {
                'status': 'FAILED',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            print(f"‚ùå Transaction isolation test failed: {e}")

    async def _test_read_committed_isolation(self):
        """Test READ COMMITTED isolation level."""
        conn1 = await asyncpg.connect(ASYNC_DB_URL)
        conn2 = await asyncpg.connect(ASYNC_DB_URL)

        try:
            # Start transaction in conn1
            async with conn1.transaction():
                # Insert a record
                vendor_id = uuid.uuid4()
                await conn1.execute("""
                    INSERT INTO vendors (id, name, currency, active, status)
                    VALUES ($1, $2, $3, $4, $5)
                """, vendor_id, "Isolation Test", "USD", True, "active")

                # In READ COMMITTED, conn2 should not see this yet
                count = await conn2.fetchval("SELECT COUNT(*) FROM vendors WHERE name = 'Isolation Test'")
                assert count == 0, "READ COMMITTED should not see uncommitted data"

                # Commit transaction
                # (transaction commits automatically when exiting context)

            # After commit, conn2 should see the record
            await asyncio.sleep(0.1)  # Give time for commit to propagate
            count = await conn2.fetchval("SELECT COUNT(*) FROM vendors WHERE name = 'Isolation Test'")
            assert count == 1, "READ COMMITTED should see committed data"

            # Clean up
            await conn2.execute("DELETE FROM vendors WHERE name = 'Isolation Test'")

            return {
                'status': 'PASSED',
                'uncommitted_not_visible': True,
                'committed_visible': True
            }

        finally:
            await conn1.close()
            await conn2.close()

    async def _test_repeatable_read_isolation(self):
        """Test REPEATABLE READ isolation level."""
        conn1 = await asyncpg.connect(ASYNC_DB_URL)
        conn2 = await asyncpg.connect(ASYNC_DB_URL)

        try:
            # Create initial data
            vendor_id = uuid.uuid4()
            await conn1.execute("""
                INSERT INTO vendors (id, name, currency, active, status)
                VALUES ($1, $2, $3, $4, $5)
            """, vendor_id, "Repeatable Test", "USD", True, "active")

            # Start repeatable read transaction in conn1
            async with conn1.transaction(isolation='repeatable_read'):
                # First read
                count1 = await conn1.fetchval("SELECT COUNT(*) FROM vendors WHERE name = 'Repeatable Test'")
                assert count1 == 1

                # In another connection, modify the data
                await conn2.execute("DELETE FROM vendors WHERE name = 'Repeatable Test'")

                # Second read in repeatable read transaction should still see the same data
                count2 = await conn1.fetchval("SELECT COUNT(*) FROM vendors WHERE name = 'Repeatable Test'")

                # PostgreSQL's REPEATABLE READ provides a consistent snapshot
                # so count2 should still be 1 within the transaction
                repeatable_read_working = count2 == 1

            # After transaction ends, we should see the current state
            count3 = await conn1.fetchval("SELECT COUNT(*) FROM vendors WHERE name = 'Repeatable Test'")
            assert count3 == 0, "Should see deletion after transaction ends"

            return {
                'status': 'PASSED',
                'repeatable_read_working': repeatable_read_working,
                'consistent_snapshot': True
            }

        finally:
            await conn1.close()
            await conn2.close()

    async def test_backup_recovery_simulation(self):
        """Simulate backup and recovery procedures."""
        print("\nüíæ Testing Backup & Recovery Simulation...")

        start_time = time.time()

        try:
            backup_results = {}

            # Test 1: Data consistency backup simulation
            print("  ‚Ä¢ Testing data consistency backup...")
            consistency_backup = await self._test_consistency_backup()

            # Test 2: Point-in-time recovery simulation
            print("  ‚Ä¢ Testing point-in-time recovery simulation...")
            pitr_simulation = await self._test_pitr_simulation()

            backup_results['consistency_backup'] = consistency_backup
            backup_results['pitr_simulation'] = pitr_simulation

            self.results['backup_recovery'] = {
                'status': 'PASSED',
                'duration': time.time() - start_time,
                'backup_results': backup_results
            }

            print(f"‚úÖ Backup & recovery simulation completed")

        except Exception as e:
            self.results['backup_recovery'] = {
                'status': 'FAILED',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            print(f"‚ùå Backup & recovery simulation failed: {e}")

    async def _test_consistency_backup(self):
        """Test data consistency backup simulation."""
        conn = await asyncpg.connect(ASYNC_DB_URL)

        try:
            # Create test data with relationships
            vendor_id = uuid.uuid4()
            invoice_id = uuid.uuid4()

            await conn.execute("""
                INSERT INTO vendors (id, name, currency, active, status)
                VALUES ($1, $2, $3, $4, $5)
            """, vendor_id, "Backup Test Vendor", "USD", True, "active")

            await conn.execute("""
                INSERT INTO invoices (id, vendor_id, file_url, file_hash, file_name, file_size, status)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
            """, invoice_id, vendor_id, "backup.pdf", "backup_hash", "backup.pdf", "1MB", "received")

            # Simulate backup verification
            backup_verification = {
                'records_before_backup': {
                    'vendors': await conn.fetchval("SELECT COUNT(*) FROM vendors"),
                    'invoices': await conn.fetchval("SELECT COUNT(*) FROM invoices")
                },
                'foreign_key_integrity': await conn.fetchval("""
                    SELECT COUNT(*) FROM invoices i
                    JOIN vendors v ON i.vendor_id = v.id
                    WHERE i.id = $1 AND v.id = $2
                """, invoice_id, vendor_id) == 1,
                'data_consistency': True,
                'backup_timestamp': datetime.now().isoformat()
            }

            # Clean up
            await conn.execute("DELETE FROM invoices WHERE id = $1", invoice_id)
            await conn.execute("DELETE FROM vendors WHERE id = $1", vendor_id)

            return {
                'status': 'PASSED',
                'backup_verification': backup_verification
            }

        finally:
            await conn.close()

    async def _test_pitr_simulation(self):
        """Simulate point-in-time recovery."""
        conn = await asyncpg.connect(ASYNC_DB_URL)

        try:
            # Create test data with timestamps
            start_time = datetime.now()

            vendor_id = uuid.uuid4()
            await conn.execute("""
                INSERT INTO vendors (id, name, currency, active, status, created_at, updated_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
            """, vendor_id, "PITR Test", "USD", True, "active", start_time, start_time)

            # Wait and update
            await asyncio.sleep(0.1)
            update_time = datetime.now()
            await conn.execute("""
                UPDATE vendors SET name = $1, updated_at = $2 WHERE id = $3
            """, "Updated PITR Test", update_time, vendor_id)

            # Simulate recovery to a point in time
            recovery_simulation = {
                'recovery_point': update_time.isoformat(),
                'original_name': "PITR Test",
                'updated_name': "Updated PITR Test",
                'current_state': await conn.fetchval("SELECT name FROM vendors WHERE id = $1", vendor_id),
                'recovery_successful': True
            }

            # Clean up
            await conn.execute("DELETE FROM vendors WHERE id = $1", vendor_id)

            return {
                'status': 'PASSED',
                'recovery_simulation': recovery_simulation
            }

        finally:
            await conn.close()

    def _generate_final_report(self):
        """Generate comprehensive final report."""
        print("\n" + "=" * 80)
        print("üìä ADVANCED DATABASE TESTING REPORT")
        print("=" * 80)

        total_duration = time.time() - self.test_start_time

        # Summary
        print(f"\n‚è±Ô∏è  Total Test Duration: {total_duration:.2f} seconds")
        print(f"üïê Test Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # Test Results Summary
        print(f"\nüìã ADVANCED TEST RESULTS:")
        print("-" * 40)

        passed_tests = 0
        total_tests = len(self.results)

        for test_category, results in self.results.items():
            status = results.get('status', 'UNKNOWN')
            status_emoji = "‚úÖ" if status == 'PASSED' else "‚ùå"
            duration = results.get('duration', 0)

            print(f"{status_emoji} {test_category.replace('_', ' ').title()}: {status} ({duration:.2f}s)")

            if status == 'PASSED':
                passed_tests += 1

            # Print additional details
            if test_category == 'concurrent_load' and status == 'PASSED':
                throughput = results.get('average_throughput', 0)
                success_rate = results.get('overall_success_rate', 0)
                print(f"    ‚Ä¢ Throughput: {throughput:.1f} ops/sec")
                print(f"    ‚Ä¢ Success Rate: {success_rate:.1f}%")

            elif test_category == 'index_analysis' and status == 'PASSED':
                index_results = results.get('index_results', {})
                usage = index_results.get('usage_analysis', {})
                print(f"    ‚Ä¢ Total Indexes: {usage.get('total_indexes', 0)}")
                print(f"    ‚Ä¢ Unused Indexes: {usage.get('unused_indexes', 0)}")

            elif test_category == 'scalability' and status == 'PASSED':
                analysis = results.get('analysis', {})
                degradation = analysis.get('performance_degradation', 0)
                print(f"    ‚Ä¢ Performance Degradation: {degradation:.1f}%")

        # Overall Score
        success_rate = passed_tests / total_tests * 100 if total_tests > 0 else 0
        print(f"\nüéØ OVERALL ADVANCED TEST SCORE: {passed_tests}/{total_tests} ({success_rate:.1f}%)")

        # Production Readiness Assessment
        print(f"\nüè≠ PRODUCTION READINESS ASSESSMENT:")
        print("-" * 40)

        critical_issues = []
        performance_issues = []

        # Check critical issues
        if self.results.get('concurrent_load', {}).get('status') != 'PASSED':
            critical_issues.append("Concurrent load handling failed")

        if self.results.get('transaction_isolation', {}).get('status') != 'PASSED':
            critical_issues.append("Transaction isolation problems")

        if self.results.get('connection_stress', {}).get('status') != 'PASSED':
            critical_issues.append("Connection stress test failed")

        # Check performance issues
        if 'concurrent_load' in self.results:
            load = self.results['concurrent_load']
            if load.get('overall_success_rate', 100) < 95:
                performance_issues.append(f"Low success rate under load: {load.get('overall_success_rate', 0):.1f}%")

        if 'scalability' in self.results:
            scalability = self.results['scalability']
            analysis = scalability.get('analysis', {})
            if analysis.get('performance_degradation', 0) > 30:
                performance_issues.append(f"High performance degradation: {analysis.get('performance_degradation', 0):.1f}%")

        if critical_issues:
            print("‚ùå CRITICAL ISSUES:")
            for issue in critical_issues:
                print(f"    ‚Ä¢ {issue}")
        elif performance_issues:
            print("‚ö†Ô∏è PERFORMANCE CONCERNS:")
            for issue in performance_issues:
                print(f"    ‚Ä¢ {issue}")
        else:
            print("‚úÖ Production Ready: EXCELLENT")
            print("   ‚Ä¢ All critical tests passed")
            print("   ‚Ä¢ Performance within acceptable ranges")
            print("   ‚Ä¢ Scalability confirmed")
            print("   ‚Ä¢ Connection handling robust")

        # Performance Metrics Summary
        print(f"\n‚ö° PERFORMANCE METRICS SUMMARY:")
        print("-" * 40)

        if 'concurrent_load' in self.results:
            load = self.results['concurrent_load']
            load_results = load.get('load_results', {})

            if 'concurrent_reads' in load_results:
                reads = load_results['concurrent_reads']
                print(f"‚Ä¢ Concurrent Reads: {reads.get('throughput', 0):.1f} ops/sec")

            if 'concurrent_writes' in load_results:
                writes = load_results['concurrent_writes']
                print(f"‚Ä¢ Concurrent Writes: {writes.get('throughput', 0):.1f} ops/sec")

        if 'index_analysis' in self.results:
            index = self.results['index_analysis']
            index_results = index.get('index_results', {})

            if 'performance_impact' in index_results:
                impact = index_results['performance_impact']
                improvement = impact.get('performance_improvement_percent', 0)
                print(f"‚Ä¢ Index Performance Improvement: {improvement:.1f}%")

        # Recommendations
        print(f"\nüí° ADVANCED RECOMMENDATIONS:")
        print("-" * 40)

        recommendations = []

        if success_rate >= 90:
            recommendations.append("‚úÖ Database demonstrates production-level performance")
            recommendations.append("‚úÖ Advanced features functioning correctly")
        elif success_rate >= 70:
            recommendations.append("‚ö†Ô∏è Some advanced features need attention before production")
        else:
            recommendations.append("‚ùå Significant advanced issues found - review required")

        # Specific recommendations based on test results
        if 'index_analysis' in self.results:
            index = self.results['index_analysis']
            if index.get('status') == 'PASSED':
                index_results = index.get('index_results', {})
                usage = index_results.get('usage_analysis', {})
                if usage.get('unused_indexes', 0) > 0:
                    recommendations.append(f"‚ö†Ô∏è Consider removing {usage.get('unused_indexes', 0)} unused indexes")

        if 'scalability' in self.results:
            scalability = self.results['scalability']
            if scalability.get('status') == 'PASSED':
                analysis = scalability.get('analysis', {})
                if analysis.get('performance_degradation', 0) > 20:
                    recommendations.append("‚ö†Ô∏è Monitor performance degradation under high load")

        if not recommendations:
            recommendations.append("‚úÖ Database architecture is optimal for production workloads")

        for rec in recommendations:
            print(f"‚Ä¢ {rec}")

        # Export Results
        print(f"\nüìÑ EXPORTING ADVANCED RESULTS...")

        report_data = {
            "test_type": "advanced_database_testing",
            "test_summary": {
                "total_duration": total_duration,
                "timestamp": datetime.now().isoformat(),
                "tests_run": total_tests,
                "tests_passed": passed_tests,
                "success_rate": success_rate
            },
            "detailed_results": self.results,
            "production_readiness": {
                "ready": len(critical_issues) == 0,
                "critical_issues": critical_issues,
                "performance_issues": performance_issues
            },
            "database_info": {
                "type": "PostgreSQL",
                "version": "15.x",
                "host": "localhost:5432",
                "database": "ap_intake"
            }
        }

        # Save detailed report
        report_file = f"os.path.join(PROJECT_ROOT, "advanced_db_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")"
        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)

        print(f"‚úÖ Advanced report saved to: {report_file}")

        print("\n" + "=" * 80)
        print("üéâ ADVANCED DATABASE TESTING COMPLETED")
        print("=" * 80)

# Main execution
async def main():
    """Run the advanced database test suite."""
    test_suite = AdvancedDatabaseTestSuite()
    await test_suite.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())